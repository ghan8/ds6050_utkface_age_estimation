{"cells":[{"source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models \nfrom torchvision.models import vision_transformer, vit_b_16\nimport os\n\nclass Model(nn.Module):\n    def __init__(self, model_name, version, num_classes, pretrained=True):\n        super(Model, self).__init__()\n        \n        self.model_name = model_name  # Store the model name as an attribute\n        \n        if pretrained == True:\n            transfer = \"pretrained\"\n        else:\n            transfer = \"scratch\"\n        \n        if model_name == 'vgg16':\n            # Load the pretrained vgg16 model\n            vgg16 = torch.hub.load('pytorch/vision:v0.9.0', 'vgg16', pretrained=pretrained)\n            # Freeze training for all layers\n            for param in vgg16.features.parameters(): \n                param.require_grad = False\n            # Newly created modules have require_grad=True by default\n            num_features = vgg16.classifier[6].in_features\n            # Remove last layer\n            features = list(vgg16.classifier.children())[:-1]\n            # Add our layer with num classes\n            features.extend([nn.Linear(num_features, num_classes)])\n            # Replace the model classifier\n            vgg16.classifier = nn.Sequential(*features)\n\t\t\t\n            # construct the model file name to load trained weights\n            model_file_name = f\"{model_name}_{transfer}_{version}.pth\"\n            #if os.path.exists(model_file_name):\n            #    vgg16.load_state_dict(torch.load(model_file_name))\n            \n            self.model = vgg16\n            \n        elif model_name == 'resnet50':\n            # Load the pretrained resnet50 model\n            resnet50 = torch.hub.load('pytorch/vision:v0.9.0', 'resnet50', pretrained=pretrained)\n            # Freeze training for all layers except the last one\n            for param in resnet50.parameters():\n                param.requires_grad = False\n            # Newly created modules have require_grad=True by default\n            # Replace the last layer with a new layer\n            resnet50.fc = nn.Sequential(\n                nn.Linear(resnet50.fc.in_features, 4096),\n                nn.ReLU(inplace=True),\n                nn.Dropout(p=0.5, inplace=False),\n                nn.Linear(4096, 4096),\n                nn.ReLU(inplace=True),\n                nn.Dropout(p=0.5, inplace=False),\n                nn.Linear(4096, num_classes)\n            )\n            # construct the model file name to load trained weights\n            model_file_name = f\"{model_name}_{transfer}_{version}.pth\"\n            #if os.path.exists(model_file_name):\n            #    resnet50.load_state_dict(torch.load(model_file_name))\n            self.model = resnet50\n            \n        elif model_name == 'vitb16':\n            if pretrained == True:\n            \tvitb16 = models.vit_b_16(weights='IMAGENET1K_SWAG_LINEAR_V1')\n            else: \n                vitb16 = models.vit_b_16()\n                \n            # Freeze training for all layers except the last one\n            for param in vitb16.parameters():\n                param.requires_grad = False\n            \n            # Extend the model.head with another layer for 116 class output classification\n            vitb16.heads = nn.Sequential(\n                 vitb16.heads.head,\n                 nn.Linear(in_features=vitb16.heads.head.out_features, out_features=4096),\n                 nn.ReLU(inplace=True),\n                 nn.Dropout(p=0.5, inplace=False),\n                 nn.Linear(4096, 4096),\n                 nn.ReLU(inplace=True),\n                 nn.Dropout(p=0.5, inplace=False),\n                 nn.Linear(4096, num_classes)\n             )\n\n            # construct the model file name to load trained weights\n            model_file_name = f\"{model_name}_{transfer}_{version}.pth\"\n            #if os.path.exists(model_file_name):\n            #    vitb16.load_state_dict(torch.load(model_file_name))\n                \n            self.model = vitb16\n        \n        else:\n            raise ValueError(\"Unsupported model name. Choose from 'vgg16', 'resnet50', '....'.\")\n    \n    def forward(self, x):\n        return self.model(x)\n    \n    def freeze_layers(self):\n        for param in self.model.parameters():\n            param.requires_grad = False\n    \n    def unfreeze_layers(self):\n        for param in self.model.parameters():\n            param.requires_grad = True\n","metadata":{"executionCancelledAt":null,"executionTime":2501,"lastExecutedAt":1701748278971,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import torch\nimport torch.nn as nn\nimport torchvision.models as models \nfrom torchvision.models import vision_transformer, vit_b_16\nimport os\n\nclass Model(nn.Module):\n    def __init__(self, model_name, version, num_classes, pretrained=True):\n        super(Model, self).__init__()\n        \n        self.model_name = model_name  # Store the model name as an attribute\n        \n        if pretrained == True:\n            transfer = \"pretrained\"\n        else:\n            transfer = \"scratch\"\n        \n        if model_name == 'vgg16':\n            # Load the pretrained vgg16 model\n            vgg16 = torch.hub.load('pytorch/vision:v0.9.0', 'vgg16', pretrained=pretrained)\n            # Freeze training for all layers\n            for param in vgg16.features.parameters(): \n                param.require_grad = False\n            # Newly created modules have require_grad=True by default\n            num_features = vgg16.classifier[6].in_features\n            # Remove last layer\n            features = list(vgg16.classifier.children())[:-1]\n            # Add our layer with num classes\n            features.extend([nn.Linear(num_features, num_classes)])\n            # Replace the model classifier\n            vgg16.classifier = nn.Sequential(*features)\n\t\t\t\n            # construct the model file name to load trained weights\n            model_file_name = f\"{model_name}_{transfer}_{version}.pth\"\n            #if os.path.exists(model_file_name):\n            #    vgg16.load_state_dict(torch.load(model_file_name))\n            \n            self.model = vgg16\n            \n        elif model_name == 'resnet50':\n            # Load the pretrained resnet50 model\n            resnet50 = torch.hub.load('pytorch/vision:v0.9.0', 'resnet50', pretrained=pretrained)\n            # Freeze training for all layers except the last one\n            for param in resnet50.parameters():\n                param.requires_grad = False\n            # Newly created modules have require_grad=True by default\n            # Replace the last layer with a new layer\n            resnet50.fc = nn.Sequential(\n                nn.Linear(resnet50.fc.in_features, 4096),\n                nn.ReLU(inplace=True),\n                nn.Dropout(p=0.5, inplace=False),\n                nn.Linear(4096, 4096),\n                nn.ReLU(inplace=True),\n                nn.Dropout(p=0.5, inplace=False),\n                nn.Linear(4096, num_classes)\n            )\n            # construct the model file name to load trained weights\n            model_file_name = f\"{model_name}_{transfer}_{version}.pth\"\n            #if os.path.exists(model_file_name):\n            #    resnet50.load_state_dict(torch.load(model_file_name))\n            self.model = resnet50\n            \n        elif model_name == 'vitb16':\n            if pretrained == True:\n            \tvitb16 = models.vit_b_16(weights='IMAGENET1K_SWAG_LINEAR_V1')\n            else: \n                vitb16 = models.vit_b_16()\n                \n            # Freeze training for all layers except the last one\n            for param in vitb16.parameters():\n                param.requires_grad = False\n            \n            # Extend the model.head with another layer for 116 class output classification\n            vitb16.heads = nn.Sequential(\n                 vitb16.heads.head,\n                 nn.Linear(in_features=vitb16.heads.head.out_features, out_features=4096),\n                 nn.ReLU(inplace=True),\n                 nn.Dropout(p=0.5, inplace=False),\n                 nn.Linear(4096, 4096),\n                 nn.ReLU(inplace=True),\n                 nn.Dropout(p=0.5, inplace=False),\n                 nn.Linear(4096, num_classes)\n             )\n\n            # construct the model file name to load trained weights\n            model_file_name = f\"{model_name}_{transfer}_{version}.pth\"\n            #if os.path.exists(model_file_name):\n            #    vitb16.load_state_dict(torch.load(model_file_name))\n                \n            self.model = vitb16\n        \n        else:\n            raise ValueError(\"Unsupported model name. Choose from 'vgg16', 'resnet50', '....'.\")\n    \n    def forward(self, x):\n        return self.model(x)\n    \n    def freeze_layers(self):\n        for param in self.model.parameters():\n            param.requires_grad = False\n    \n    def unfreeze_layers(self):\n        for param in self.model.parameters():\n            param.requires_grad = True\n","outputsMetadata":{"0":{"height":37,"type":"stream"},"1":{"height":37,"type":"stream"},"2":{"height":37,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"cell_type":"code","id":"97461814-6e1d-4608-b8dd-15d939bfbf9d","execution_count":3,"outputs":[]},{"source":"# Example usage:\nnum_classes = 116  # Replace with the number of classes in your task\nmodel_name = 'vgg16'  # Choose the desired model\npretrained = True  # Use pretrained weights (can be set to False to start with random weights)\nversion = 1 \n\n# intantiate the desired model from the model class\nmodel = Model(model_name, version,  num_classes, pretrained)\n\n# Access the model name attribute\nprint(\"Model Name:\", model.model_name)\n\n# Freeze all layers\n# for param in model.parameters():\n#     param.requires_grad = False\nmodel.freeze_layers()\n\n# Unfreeze all layers\n# for param in model.parameters():\n#     param.requires_grad = True\nmodel.unfreeze_layers()\n\n# load model weights\nmodel_file_name = \"vgg16_pretrained_1.pth\"\nmodel.load_state_dict(torch.load(model_file_name))\n","metadata":{"executionCancelledAt":null,"executionTime":3578,"lastExecutedAt":1701748446799,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Example usage:\nnum_classes = 116  # Replace with the number of classes in your task\nmodel_name = 'vgg16'  # Choose the desired model\npretrained = True  # Use pretrained weights (can be set to False to start with random weights)\nversion = 1 \n\n# intantiate the desired model from the model class\nmodel = Model(model_name, version,  num_classes, pretrained)\n\n# Access the model name attribute\nprint(\"Model Name:\", model.model_name)\n\n# Freeze all layers\n# for param in model.parameters():\n#     param.requires_grad = False\nmodel.freeze_layers()\n\n# Unfreeze all layers\n# for param in model.parameters():\n#     param.requires_grad = True\nmodel.unfreeze_layers()\n\n# load model weights\nmodel_file_name = \"vgg16_pretrained_1.pth\"\nmodel.load_state_dict(torch.load(model_file_name))\n","outputsMetadata":{"0":{"height":37,"type":"stream"},"1":{"height":37,"type":"stream"},"2":{"height":37,"type":"stream"}}},"cell_type":"code","id":"ed0bd224-ef0d-48e5-991b-e9403ac100a7","execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":"Using cache found in /home/repl/.cache/torch/hub/pytorch_vision_v0.9.0\n"},{"output_type":"stream","name":"stdout","text":"Model Name: vgg16\n"},{"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{},"execution_count":6}]},{"source":"# import torch\n# from torchvision.models import vision_transformer, vit_h_14\n\n# # Load the pretrained ViT_H_14 model\n# model = vit_h_14(weights='IMAGENET1K_SWAG_LINEAR_V1')\n\n# # Set the model to evaluation mode\n# model.eval()\n\n# # # Extend the model.head with another layer for 106 class output classification\n# model.heads = torch.nn.Sequential(\n#     model.heads.head,\n#     torch.nn.Linear(in_features=model.heads.head.out_features, out_features=106)\n# )\n\n# # Unfreeze the heads weights\n# for param in model.heads.parameters():\n#     param.requires_grad = True\n\n# vith14 = model}","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":true}},"cell_type":"code","id":"56f6adaa-e29d-4659-b2a6-3c5d4f441596","execution_count":null,"outputs":[]},{"source":"# # ViT_B_16_Weights.IMAGENET1K_SWAG_LINEAR_V1\n# import torch\n# from torchvision.models import vision_transformer, vit_b_16\n\n# # Load the pretrained ViT_H_14 model\n# model = vit_b_16(weights='IMAGENET1K_SWAG_LINEAR_V1')\n\n# # Set the model to evaluation mode\n# model.eval()\n\n# # # Extend the model.head with another layer for 116 class output classification\n# model.heads = torch.nn.Sequential(\n#     model.heads.head,\n#     torch.nn.Linear(in_features=model.heads.head.out_features, out_features=116)\n# )\n\n# # Unfreeze the heads weights\n# for param in model.heads.parameters():\n#     param.requires_grad = True\n\n# vitb16 = model\n\n# model_name = \"vitb16_pretrained_1.pth\"\n# if os.path.isfile(model_name):\n#     model.load_state_dict(torch.load(model_name))","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":true}},"cell_type":"code","id":"3e2bb664-4b26-4542-9692-8c4bdb55c93e","execution_count":null,"outputs":[]},{"source":"# mobilenetv3 = torch.hub.load('pytorch/vision:v0.9.0', 'mobilenet_v3_large', pretrained=True)\n# for param in mobilenetv3.features.parameters(): \n#     param.require_grad = False\n# num_features = mobilenetv3.classifier[3].in_features\n# features = list(mobilenetv3.classifier.children())[:-1]\n# features.extend([\n#     nn.Linear(num_features, 4096),\n#     nn.ReLU(inplace=True),\n#     nn.Dropout(p=0.5, inplace=False),\n#     nn.Linear(4096, 116)\n# ])\n# mobilenetv3.classifier = nn.Sequential(*features)\n# mobilenetv3","metadata":{"executionCancelledAt":null,"executionTime":74,"lastExecutedAt":1701748489870,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# mobilenetv3 = torch.hub.load('pytorch/vision:v0.9.0', 'mobilenet_v3_large', pretrained=True)\n# for param in mobilenetv3.features.parameters(): \n#     param.require_grad = False\n# num_features = mobilenetv3.classifier[3].in_features\n# features = list(mobilenetv3.classifier.children())[:-1]\n# features.extend([\n#     nn.Linear(num_features, 4096),\n#     nn.ReLU(inplace=True),\n#     nn.Dropout(p=0.5, inplace=False),\n#     nn.Linear(4096, 116)\n# ])\n# mobilenetv3.classifier = nn.Sequential(*features)\n# mobilenetv3","outputsMetadata":{"0":{"height":37,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":true}},"cell_type":"code","id":"6044dbdc-5093-4434-8ad7-8cf9ada7b263","execution_count":7,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}